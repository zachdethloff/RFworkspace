{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1decd83-ab89-4fa4-a906-5edc6c68da53",
   "metadata": {},
   "source": [
    "# 🎞️ Bonus: SAR Video & Subaperture Processing Magic ✨\n",
    "\n",
    "Alright, by now we've learned how to generate **high-resolution SAR images** by combining **measurements** collected along the synthetic aperture. 📡🖼️  \n",
    "\n",
    "We’ve also seen how we can divide the aperture into **smaller sub-apertures** to create multiple **“look” images**, which we then **average** to reduce speckle. 🎲➕🧩\n",
    "\n",
    "---\n",
    "\n",
    "## 🚪 Opening the Door to New Possibilities\n",
    "\n",
    "But here’s the exciting part — this concept of **sub-aperture division** isn’t just useful for reducing speckle. It also unlocks some fascinating **exploitation techniques**. 🔓💡\n",
    "\n",
    "Why? Because **each sub-aperture corresponds to measurements collected at a different time** as the radar platform moves during the collection. ⏱️\n",
    "\n",
    "That means we actually capture the **temporal evolution** of the scene — how reflections **change over time**! 🔁\n",
    "\n",
    "---\n",
    "\n",
    "## 🎬 Enter: SAR Video\n",
    "\n",
    "This leads us straight to the idea of a **SAR video**. 🎥\n",
    "\n",
    "The method is **similar to multilooking** in terms of dividing the synthetic aperture into segments. But this time, we take a different approach:\n",
    "\n",
    "- ❌ Instead of **averaging** the looks together...  \n",
    "- ✅ We treat **each look as a frame** in a video!  \n",
    "\n",
    "The result? A SAR-based video that **visualizes how the radar echoes evolve** over the course of the data collection. You’re essentially watching the scene unfold as the radar sees it — one sub-aperture at a time. 🛰️👁️‍🗨️🎞️\n",
    "\n",
    "---\n",
    "\n",
    "🎥 Let’s experiment with the **SAR video**, using a few **range-compressed datasets** from **Paris 🇫🇷** and **Rome 🇮🇹**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb61ed66-700d-4bf5-aabe-c9f5c4ec34d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy.interpolate import interp1d\n",
    "c = 299792458 # Speed of light (m/s)\n",
    "\n",
    "def calculate_image_pixel_positions(scene_center_pos_ecef, center_of_aperture_pos, x_extent, y_extent, nx, ny):\n",
    "    \"\"\"\n",
    "    Calculate image pixel coordinate positions in ECEF system.\n",
    "    \"\"\"\n",
    "    # Construct local tangent-plane basis\n",
    "    scene_center_pos = np.asarray(scene_center_pos_ecef, dtype=float)  # Scene center\n",
    "\n",
    "    # z-axis: a vector perpendicular to the image (x,y) plane (surface normal)\n",
    "    z_hat = scene_center_pos / np.linalg.norm(scene_center_pos)\n",
    "\n",
    "    # define the line-of-sight vector from scene center to center of aperture\n",
    "    slant_range_vector = scene_center_pos - center_of_aperture_pos \n",
    "\n",
    "    # Project it so that y-axis (ground-range) is purely horizontal along the ground plane\n",
    "    ground_range_vector = slant_range_vector - np.dot(slant_range_vector, z_hat)*z_hat\n",
    "    ground_range_unit_vector = np.linalg.norm(ground_range_vector)\n",
    "\n",
    "    # y-axis is ground range\n",
    "    y_hat = ground_range_vector / np.linalg.norm(ground_range_vector)\n",
    "\n",
    "    # x-axis is cross-range (right-handed system), perpendicular to both y- and z-axis\n",
    "    x_hat = np.cross(y_hat, z_hat)\n",
    "    x_hat /= np.linalg.norm(x_hat)\n",
    "\n",
    "    # Build local coordinate grid\n",
    "    x_grid = np.linspace(x_extent[0], x_extent[1], nx)\n",
    "    y_grid = np.linspace(y_extent[0], y_extent[1], ny)\n",
    "    \n",
    "    # Build an array of local coordinates, then convert to ECEF using scene center and image axis vectors\n",
    "    XX, YY = np.meshgrid(x_grid, y_grid)  # shape (ny, nx) each\n",
    "    # Flatten => shape (num_pixels,)\n",
    "    X_flat = XX.ravel()\n",
    "    Y_flat = YY.ravel()\n",
    "\n",
    "    # Compute ECEF for each pixel\n",
    "    scene_center = scene_center_pos.reshape((1,3)) \n",
    "    x_axis = x_hat.reshape((1,3)) # image cross-range axis\n",
    "    y_axis = y_hat.reshape((1,3)) # image range axis\n",
    "\n",
    "    # We use the x and y grids to compute the pixel positions in one go\n",
    "    X_col = X_flat.reshape((-1,1))   # shape (num_pixels,1)\n",
    "    Y_col = Y_flat.reshape((-1,1))   # shape (num_pixels,1)\n",
    "\n",
    "    # This is an array of size (num_pixels,3) that has the (x,y,z) ECEF coordinates for each pixel location\n",
    "    pixel_pos_ecef = scene_center + X_col*x_axis + Y_col*y_axis\n",
    "\n",
    "    return pixel_pos_ecef, x_grid, y_grid\n",
    "\n",
    "def sar_beamforming(\n",
    "    slant_ranges,                   # 1D array of range bins in meters (shape = (num_range_samples,))\n",
    "    range_compressed_data,          # Complex SAR data, shape = (num_pulses, num_range_samples)\n",
    "    radar_pos_vs_time_ecef,         # Radar positions (num_pulses, 3) in ECEF\n",
    "    center_of_aperture_pos,         # Radar position at the center of aperture (1, 3) in ECEF\n",
    "    scene_center_pos_ecef,          # Scene center (3,) in ECEF\n",
    "    wavelength,                     # Radar carrier wavelength (m)\n",
    "    x_extent,                       # X (cross-range) extent in local coords around scene center (m)\n",
    "    y_extent,                       # Y (ground range) extent in local coords around scene center (m)\n",
    "    pixel_spacing_x,                             # Pixel spacing in x, meters (cross-range)\n",
    "    pixel_spacing_y                              # Pixel spacing in in y, meters (ground range)\n",
    "):\n",
    "    \"\"\"\n",
    "    SAR Beamforming algorithm that:\n",
    "      1) Builds a local tangent plane around 'scene_center_pos_ecef'.\n",
    "         - z-axis is \"up\"\n",
    "         - y-axis is ground range, along the line from scene center -> radar position at the mid-aperture point\n",
    "         - x-axis is cross-range, along the filght path, completes a right-handed system\n",
    "      2) Maps each local (x, y) pixel to ECEF postion (we need to have pixels and radar position in the same coordinate system)\n",
    "      3) Uses backprojection beamforming to sum signals coherently to form each focused pixel value.\n",
    "\n",
    "    Returns:\n",
    "      sar_image:   2D complex array (ny, nx) with the focused image\n",
    "      x_grid:  1D array of local x-coords\n",
    "      y_grid:  1D array of local y-coords\n",
    "    \"\"\"\n",
    "\n",
    "    x_size_m = np.abs(x_extent[-1] - x_extent[0]) # Image size in cross-range (m)\n",
    "    y_size_m = np.abs(y_extent[-1] - y_extent[0]) # Image size in ground-range (m)\n",
    "    nx = int(round( x_size_m / pixel_spacing_x )) # Number of pixels in cross-range\n",
    "    ny = int(round( y_size_m / pixel_spacing_y )) # Number of pixels in ground-range\n",
    "    \n",
    "    # pixel_pos_ecef is an array that has the (x,y,z) ECEF coordinates for each pixel position\n",
    "    pixel_pos_ecef, x_grid, y_grid = calculate_image_pixel_positions(\n",
    "        scene_center_pos_ecef, \n",
    "        center_of_aperture_pos, \n",
    "        x_extent, \n",
    "        y_extent, \n",
    "        nx, \n",
    "        ny)\n",
    "\n",
    "    # We'll accumulate the phasor sum for each pixel in a 1D array\n",
    "    image_array_flat = np.zeros(pixel_pos_ecef.shape[0], dtype=np.complex128)\n",
    "\n",
    "    # For each pulse, do vector calculations using the pixel array\n",
    "    last_printed_percent = -1\n",
    "    number_of_pulses = range_compressed_data.shape[0]\n",
    "    for i_pulse in range(number_of_pulses):\n",
    "        \n",
    "        # Radar position at this pulse\n",
    "        radar_xyz = radar_pos_vs_time_ecef[i_pulse, :]\n",
    "        \n",
    "        # Lets calculate an array that has the distance between each pixel and the radar at this pulse\n",
    "        radar_to_pixel_vec = pixel_pos_ecef - radar_xyz # vectors from the radar position to each pixel\n",
    "        # One way range to each pixel from this \"virtual\" element position\n",
    "        range_to_all_pixels = np.sqrt(np.sum(radar_to_pixel_vec**2, axis=1))\n",
    "\n",
    "        # This is the echo data of the current pulse\n",
    "        pulse_echo_data = range_compressed_data[i_pulse, :] \n",
    "        \n",
    "        # Pick the correct sample with cubic interpolation : This is more accurate than our previous nearest neighbor method\n",
    "        real_interpolator = interp1d(slant_ranges, np.real(pulse_echo_data), kind='cubic', fill_value=\"extrapolate\")\n",
    "        imag_interpolator = interp1d(slant_ranges, np.imag(pulse_echo_data), kind='cubic', fill_value=\"extrapolate\")\n",
    "        # Interpolate I and Q separately and form the phasors\n",
    "        echo_data_all_pixels = real_interpolator(range_to_all_pixels) + 1j*imag_interpolator(range_to_all_pixels)\n",
    "\n",
    "        # Phase correction based on the range to each pixel\n",
    "        phase_correction = 2.0 * np.pi * 2.0 * range_to_all_pixels / wavelength\n",
    "\n",
    "        # Correct phases and accumulate pixel values\n",
    "        image_array_flat += echo_data_all_pixels * np.exp(1j * phase_correction)\n",
    "        \n",
    "        # Print progress message every 5 percent, coz it's slow!        \n",
    "        progress = int(round(100.0 * i_pulse / number_of_pulses))\n",
    "        if progress % 25 == 0 and progress != last_printed_percent:\n",
    "            print(f\"SAR Beamforming Progress: {progress}%\")\n",
    "            last_printed_percent = progress\n",
    "\n",
    "    # Reshape image to to (ny,nx) and normalise amplitude\n",
    "    sar_image = image_array_flat.reshape((ny, nx)) / number_of_pulses\n",
    "\n",
    "    return sar_image, x_grid, y_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe47cab-1ce2-4101-ab36-a2fa12107145",
   "metadata": {},
   "source": [
    "### Here you can choose the dataset 🗼 🏛️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dbbfb4-763c-4681-b610-53043565917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the radar data\n",
    "#data_relative_path = Path(\"./data/range_compressed_SAR_dataset_1.npz\") # Croissants for everyone!\n",
    "data_relative_path = Path(\"./data/range_compressed_SAR_dataset_2.npz\") # Bread and circuses for everyone!\n",
    "data_path = data_relative_path.resolve()\n",
    "sar_data_dict = np.load(data_path)\n",
    "\n",
    "range_axis = sar_data_dict[\"range_vector\"]\n",
    "data_matrix = sar_data_dict[\"range_compressed_data\"]\n",
    "scene_center = sar_data_dict[\"scene_center_position\"]\n",
    "satellite_position_vs_pulse = sar_data_dict[\"satellite_position_vs_pulse\"]\n",
    "wavelength = sar_data_dict[\"wavelength\"]\n",
    "signal_bandwidth = sar_data_dict[\"chirp_bandwidth\"]\n",
    "\n",
    "number_of_pulses_in_data = data_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29dda43-31d6-4316-9b39-ebbfd28a98bb",
   "metadata": {},
   "source": [
    "🛠️ Feel free to **play around** with:\n",
    "- The **number of frames** in the video 🎞️\n",
    "- The **pixel spacing** of the image 📏🖼️\n",
    "\n",
    "See how these settings affect the final result—and enjoy exploring the data! 🌌🔍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c236aa3-6c87-4eb5-92cd-a1ce9d277cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can experiment with image sizes and amount of pixels. But be careful not to under- or oversample too much!\n",
    "x_extent=(-300, 300) # Image extent in cross-range (m)\n",
    "y_extent=(-300, 300) # Image extent in ground range (m)\n",
    "\n",
    "# Radar position at the center of synthetic aperture (used to define ground range axis)\n",
    "center_of_aperture_pos = satellite_position_vs_pulse[number_of_pulses_in_data//2,:]\n",
    "\n",
    "pixel_spacing_x = 1.5 # np.abs(x_extent[-1] - x_extent[0]) / nx\n",
    "pixel_spacing_y = 1.5 # np.abs(y_extent[-1] - y_extent[0]) / ny\n",
    "\n",
    "# If you want to reconstruct the single look image for comparison, feel free to do so\n",
    "#sar_image, x_grid, y_grid = sar_beamforming(range_axis, data_matrix, satellite_position_vs_pulse, center_of_aperture_pos, scene_center, \n",
    "#                                            wavelength, x_extent, y_extent,  pixel_spacing_x, pixel_spacing_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1b02b4-982b-4ba7-a380-9e70942a7f90",
   "metadata": {},
   "source": [
    "### Now you are in the director's seat! Let's make the SAR video! 🎥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f32d9f-810c-45af-a7c6-ba38908e423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOOSE THE NUMBER OF VIDEO FRAMES\n",
    "number_of_frames = 10\n",
    "\n",
    "# Let's determine how many pulses we have in each frame:\n",
    "nb_pulses_per_frame = number_of_pulses_in_data // number_of_frames\n",
    "\n",
    "for i_frame in range(number_of_frames):\n",
    "    print(\"Calculating video frame number\", i_frame+1, \"of\", number_of_frames)\n",
    "    # Calculate which pulses from the data to use for this look\n",
    "    pulse_start = i_frame*nb_pulses_per_frame\n",
    "    pulse_end = pulse_start + nb_pulses_per_frame\n",
    "    print(\"Start, end pulse index for this frame:\", pulse_start, pulse_end)\n",
    "    # Pick the data (i.e. the virtual elements) that correspond to this look\n",
    "    frame_data = data_matrix[pulse_start:pulse_end,:]\n",
    "    frame_radar_pos = satellite_position_vs_pulse[pulse_start:pulse_end:,:]\n",
    "    frame_image, _, _ = sar_beamforming(range_axis, frame_data, \n",
    "                                       frame_radar_pos, center_of_aperture_pos,\n",
    "                                       scene_center, wavelength, x_extent, y_extent, \n",
    "                                       pixel_spacing_x, pixel_spacing_y)\n",
    "    if i_frame == 0:\n",
    "        # Make an array for the frames\n",
    "        video_frames = np.zeros((frame_image.shape[0], frame_image.shape[1], number_of_frames))\n",
    "    # Collect the frame in the video array\n",
    "    video_frames[:,:,i_frame] = np.abs(frame_image)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79138f4d-3d2e-4521-8030-88553ac7e97d",
   "metadata": {},
   "source": [
    "### Let's play the video! 🎞️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4d7dac-c9c1-4bb9-8aa2-84054fd5e6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Adjust display. Let's use log scale to see details more clearly\n",
    "dynamic_range_dB = 25 # FEEL FREE TO ADJUST THIS VALUE TO DISPLAY THE IMAGE\n",
    "max_val = np.max(10*np.log10(np.abs(video_frames.ravel())))\n",
    "min_val = max_val - dynamic_range_dB \n",
    "\n",
    "# Create the figure and initial frame\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(10*np.log10(np.abs(video_frames[:, :, 0])), aspect='auto',\n",
    "               cmap='jet', vmin=min_val, vmax=max_val, animated=True)\n",
    "ax.axis('off')\n",
    "ax.set_title(\"SAR Video Playback\")\n",
    "\n",
    "# Update function\n",
    "def update(frame):\n",
    "    im.set_array(10*np.log10(np.abs(video_frames[:, :, frame])))\n",
    "    return [im]\n",
    "\n",
    "# Create animation\n",
    "ani = FuncAnimation(fig, update, frames=number_of_frames, interval=250, blit=False)\n",
    "\n",
    "# Display animation in notebook\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd34f3b2-fd71-432b-9069-ac517b96fabb",
   "metadata": {},
   "source": [
    "## 🌈 **Colorised Subaperture Imaging: A New Dimension of Insight**\n",
    "\n",
    "Dividing our synthetic aperture into **subapertures** doesn’t just open the door to SAR video — it also enables a fascinating way to explore how targets **reflect energy from different angles**. 🛰️🔁  \n",
    "\n",
    "Remember: each subaperture captures the scene from a **slightly different perspective** and at a **different moment in time**. This means we can examine the **angular dependence of reflectivity** — a key characteristic of many real-world targets. 📐💡\n",
    "\n",
    "---\n",
    "\n",
    "### 🪞 **Angular Reflectivity: Not All Targets Reflect Equally**\n",
    "\n",
    "Most objects on Earth do **not** reflect radar energy equally in all directions (i.e., they’re not isotropic reflectors). Instead, their brightness in SAR images depends on the **viewing angle**. Here’s a classic example:\n",
    "\n",
    "- Imagine a **flat metal plate** — like the **side of a car** or a **rooftop**.  \n",
    "- When viewed **obliquely**, radar waves reflect away (per the **law of reflection**) and **very little energy returns to the radar**.  \n",
    "- That’s why **calm water** often appears **black in SAR images** — it reflects energy away from the sensor, not back toward it. 🌊🚫📡\n",
    "\n",
    "But when the radar **views the surface head-on**, i.e., along its **surface normal**, it produces a **strong specular reflection** — resulting in a **bright spot** in the image.  \n",
    "These types of **direction-dependent reflections** are typical of **man-made structures** with regular shapes. 🏠🚗🏗️\n",
    "\n",
    "---\n",
    "\n",
    "### 🎨 **Encoding Angular Scattering in Color**\n",
    "\n",
    "One creative way to visualize this angular dependence is through **colorised subaperture imaging**:\n",
    "\n",
    "- 🔪 We divide the full synthetic aperture into **several subapertures**.  \n",
    "- 🟥🟩🟦 For example, we can split the aperture into **three segments**: beginning, middle, and end.  \n",
    "- For each segment, we create a **subaperture image** (with lower cross-range resolution than the full aperture).  \n",
    "- Then, we assign the **magnitude of each subaperture image** to a **color channel**:\n",
    "  - **Red** = early subaperture  \n",
    "  - **Green** = middle  \n",
    "  - **Blue** = late  \n",
    "\n",
    "The result? A **false-color RGB SAR image** that shows **where in the aperture the reflection occurred strongest**. 🌈🖼️\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 **How to Read a Colorised Subaperture Image**\n",
    "\n",
    "- **Gray or white pixels** = the target reflected **evenly across all subapertures**.  \n",
    "- **Red regions** = the target reflected **mostly in the first part** of the aperture (early viewing angle).  \n",
    "- **Blue regions** = the strongest return came **late in the aperture** (from a different angle).  \n",
    "- **Color transitions** in the scene tell us that **target reflectivity is changing with angle** — very useful for characterizing structures, vehicles, and complex materials. 🏢 🚙\n",
    "\n",
    "In practice, we often use **more than three subapertures** and apply a **continuous color mapping scheme**, but the core idea remains the same:  \n",
    "👉 **Color = dominant subaperture contribution** at each pixel.\n",
    "\n",
    "---\n",
    "\n",
    "This is a powerful way to visualize **directional scattering behavior** — and a beautiful example of how SAR can give us **much more than just black-and-white images**. 🌈\n",
    "\n",
    "### Let's make the color image by dividing the aperture in three color frames! 🌈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb47e31-b17c-42da-b41d-689214f0bf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's divide the aperture into three color frames\n",
    "number_of_frames = 3\n",
    "# Let's determine how many pulses we have in each frame:\n",
    "nb_pulses_per_frame = number_of_pulses_in_data // number_of_frames\n",
    "\n",
    "pixel_spacing_x = 1.5 # m\n",
    "pixel_spacing_y = 1.5 # m\n",
    "\n",
    "for i_frame in range(number_of_frames):\n",
    "    print(\"Calculating CSI frame number\", i_frame+1)\n",
    "    # Calculate which pulses from the data to use for this look\n",
    "    pulse_start = i_frame*nb_pulses_per_frame\n",
    "    pulse_end = pulse_start + nb_pulses_per_frame\n",
    "    print(\"Start, end pulse index for this frame:\", pulse_start, pulse_end)\n",
    "    # Pick the data (i.e. the virtual elements) that correspond to this look\n",
    "    frame_data = data_matrix[pulse_start:pulse_end,:]\n",
    "    frame_radar_pos = satellite_position_vs_pulse[pulse_start:pulse_end:,:]\n",
    "    frame_image, _, _ = sar_beamforming(range_axis, frame_data, \n",
    "                                       frame_radar_pos, center_of_aperture_pos,\n",
    "                                       scene_center, wavelength, x_extent, y_extent, \n",
    "                                       pixel_spacing_x, pixel_spacing_y)\n",
    "    if i_frame == 0:\n",
    "        csi_frames = np.zeros((frame_image.shape[0], frame_image.shape[1], number_of_frames))\n",
    "    # Accumulate the squared magnitude (power) of the image\n",
    "    csi_frames[:,:,i_frame] = np.abs(frame_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e63599-32e2-4738-b0d4-69d98e3ceb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def normalize_band(band):\n",
    "    \"\"\"Normalize band to [0, 1] range.\"\"\"\n",
    "    return (band - band.min()) / (band.max() - band.min() + 1e-8)\n",
    "\n",
    "def clip_and_normalize(band, pmin=0, pmax=95):\n",
    "    \"\"\"Clip extreme values and normalize to [0, 1].\"\"\"\n",
    "    vmin, vmax = np.percentile(band, [pmin, pmax])\n",
    "    band_clipped = np.clip(band, vmin, vmax)\n",
    "    return normalize_band(band_clipped)\n",
    "\n",
    "def to_log_normalized(image, pmin=0, pmax=95):\n",
    "    \"\"\"Apply log compression, clip extremes, and normalize.\"\"\"\n",
    "    log_img = 10 * np.log10(image + 1e-6)\n",
    "    return clip_and_normalize(log_img, pmin, pmax)\n",
    "\n",
    "def to_linear_normalized(image, pmin=0, pmax=95):\n",
    "    \"\"\"Clip and normalize power image without log compression.\"\"\"\n",
    "    return clip_and_normalize(image, pmin, pmax)\n",
    "\n",
    "# Compute power from complex SAR data\n",
    "power_r = np.abs(csi_frames[:, :, 0])**2\n",
    "power_g = np.abs(csi_frames[:, :, 1])**2\n",
    "power_b = np.abs(csi_frames[:, :, 2])**2\n",
    "\n",
    "# Feel free to play around with the clipping settings in the image display. \n",
    "# (They are percentages according to which we clip the histogram.)\n",
    "\n",
    "# Create log-normalized RGB image (with clipped amplitude for nicer visualization)\n",
    "log_clip_low = 15\n",
    "log_clip_high = 95\n",
    "r_log = to_log_normalized(power_r, log_clip_low, log_clip_high)\n",
    "g_log = to_log_normalized(power_g, log_clip_low, log_clip_high)\n",
    "b_log = to_log_normalized(power_b, log_clip_low, log_clip_high)\n",
    "rgb_log = np.stack((r_log, g_log, b_log), axis=-1)\n",
    "\n",
    "# Create linear-normalized RGB image (with clipped amplitude for nicer visualization)\n",
    "linear_clip_low = 0\n",
    "linear_clip_high = 96\n",
    "r_linear = to_linear_normalized(power_r, linear_clip_low, linear_clip_high)\n",
    "g_linear = to_linear_normalized(power_g, linear_clip_low, linear_clip_high)\n",
    "b_linear = to_linear_normalized(power_b, linear_clip_low, linear_clip_high)\n",
    "rgb_linear = np.stack((r_linear, g_linear, b_linear), axis=-1)\n",
    "\n",
    "# Plot: Log Scale\n",
    "plt.imshow(rgb_log, aspect='auto')\n",
    "plt.title(\"Colorized Subaperture Image (Log Scale)\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Plot: Linear Scale\n",
    "plt.imshow(rgb_linear, aspect='auto')\n",
    "plt.title(\"Colorized Subaperture Image (Linear Scale)\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fd6741-001e-4868-81f7-c8c131ad5f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
